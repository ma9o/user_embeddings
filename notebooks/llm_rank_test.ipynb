{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from json_repair import repair_json\n",
    "import os\n",
    "\n",
    "sa_info = repair_json(os.environ[\"GOOGLE_SERVICE_ACCOUNT_JSON\"], return_objects=True)\n",
    "# Create credentials object from the dictionary, specifying the required scope\n",
    "credentials = service_account.Credentials.from_service_account_info(\n",
    "    sa_info, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "\n",
    "GEMINI_EMBEDDING_MODEL_ID = \"text-embedding-large-exp-03-07\"\n",
    "# Embedding dimension\n",
    "EMBEDDING_DIMENSION = 3072  # Dimension for text-embedding-005 model\n",
    "\n",
    "embedding_client = genai.Client(\n",
    "    vertexai=True, project=\"enclaveid\", location=\"us-central1\", credentials=credentials\n",
    ")\n",
    "\n",
    "\n",
    "# Helper function to generate embeddings using Google Generative AI\n",
    "def generate_vertex_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Generates embedding for a given text using Google Generative AI.\"\"\"\n",
    "    # Use the genai library to get embeddings\n",
    "    response = embedding_client.models.embed_content(\n",
    "        model=GEMINI_EMBEDDING_MODEL_ID,\n",
    "        contents=[text],\n",
    "        config=EmbedContentConfig(\n",
    "            task_type=\"SEMANTIC_SIMILARITY\",  # Setting the task type\n",
    "            output_dimensionality=EMBEDDING_DIMENSION,  # Setting the output dimension\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Extract the embedding values from the response\n",
    "    if response and response.embeddings and len(response.embeddings) > 0:\n",
    "        embedding_values = response.embeddings[0].values\n",
    "\n",
    "        if len(embedding_values) != EMBEDDING_DIMENSION:\n",
    "            print(\n",
    "                f\"Warning: Embedding dimension mismatch. Expected {EMBEDDING_DIMENSION}, got {len(embedding_values)}\"\n",
    "            )\n",
    "            raise ValueError(\n",
    "                f\"Embedding dimension mismatch. Expected {EMBEDDING_DIMENSION}, got {len(embedding_values)}\"\n",
    "            )\n",
    "\n",
    "        return embedding_values\n",
    "    else:\n",
    "        print(\"Warning: No embedding values returned\")\n",
    "        raise ValueError(\"No embedding values returned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "TEACHER_MODEL = \"google/gemma-3-27b-it\"\n",
    "JUDGE_MODEL = \"google/gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "openrouter_client = openai.OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "def get_completion(model, data) -> str:\n",
    "    completion = openrouter_client.chat.completions.create(\n",
    "        extra_body={},\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": data},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_ndjson(\"../data/reddit/comments/RC_2015-01_prepared.ndjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/q288_3h54pg5fgrm5np7xp8h0000gn/T/ipykernel_32614/2476220988.py:3: DeprecationWarning: `DataFrame.with_row_count` is deprecated. Use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  ).with_row_count(\"row_idx\")\n"
     ]
    }
   ],
   "source": [
    "df_with_embeddings = df.with_columns(\n",
    "    embedding=pl.col(\"formatted_comment\").map_elements(\n",
    "        lambda x: generate_vertex_embedding(x),\n",
    "        strategy=\"threading\",\n",
    "        return_dtype=pl.List(pl.Float64),\n",
    "    )\n",
    ").with_row_count(\"row_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import polars as pl\n",
    "\n",
    "embeddings_np = np.array(df_with_embeddings[\"embedding\"].to_list(), dtype=\"float32\")\n",
    "faiss.normalize_L2(embeddings_np)\n",
    "cosine_index = faiss.IndexFlatIP(EMBEDDING_DIMENSION)\n",
    "cosine_index.add(embeddings_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_index(\n",
    "    query_vector: np.ndarray, k=embeddings_np.shape[0]\n",
    ") -> tuple[np.ndarray, np.ndarray] | None:\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    distances, indices = cosine_index.search(query_vector, k)\n",
    "    return distances[0][1:], indices[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_SCORE_MAP = {\n",
    "    \"Most Similar\": 4,\n",
    "    \"Highly Similar\": 3,\n",
    "    \"Somewhat Similar\": 2,\n",
    "    \"Not Similar\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_PROMPT = \"\"\"\n",
    "Your task is to assess the deep similarity between SOURCE and each CANDIDATE based on representative samples of their recent activity.\n",
    "\n",
    "Try to guess as much as possible about each user based on the following:\n",
    "- Psychological Traits & Temperament\n",
    "- Core Beliefs & Values\n",
    "- Personal History & Lived Experiences\n",
    "- Self-Concept & Internal Narrative\n",
    "- Social Roles & Relationships\n",
    "- Cultural & Group Affiliations\n",
    "- Acquired Skills, Knowledge & Abilities\n",
    "- Goals, Aspirations & Future Orientations\n",
    "\n",
    "Assign to each CANDIDATE a label from the following list:\n",
    "{label_list}\n",
    "\n",
    "Return a JSON object where the keys are the CANDIDATE user names and the values are the labels:\n",
    "{output_schema}\n",
    "\n",
    "SOURCE: {{source_user}}\n",
    "\n",
    "CANDIDATES: \n",
    "{{candidate_users}}\n",
    "\"\"\".format(\n",
    "    label_list=\", \".join([f'\"{label}\"' for label in LABEL_SCORE_MAP.keys()]),\n",
    "    output_schema=\"\"\"\n",
    "{{\n",
    "    \"user_name_1\": label_1,\n",
    "    \"user_name_2\": label_2,\n",
    "    ...\n",
    "}}\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_teacher_prompt(source_user, candidate_users):\n",
    "    return TEACHER_PROMPT.format(\n",
    "        source_user=source_user, candidate_users=candidate_users\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = \"\"\"\n",
    "You will be given a SOURCE user and a list of CANDIDATE users and the output of a smaller LLM that is trying to assess the deep similarity between the SOURCE and each CANDIDATE using these criteria:\n",
    "- Psychological Traits & Temperament\n",
    "- Core Beliefs & Values\n",
    "- Personal History & Lived Experiences\n",
    "- Self-Concept & Internal Narrative\n",
    "- Social Roles & Relationships\n",
    "- Cultural & Group Affiliations\n",
    "- Acquired Skills, Knowledge & Abilities\n",
    "- Goals, Aspirations & Future Orientations\n",
    "\n",
    "The labels are:\n",
    "{label_list}\n",
    "\n",
    "Answer wether the LLM ranking is correct or needs refinement. If so, return the correct ranking in the following JSON schema:\n",
    "{output_schema}\n",
    "\n",
    "\n",
    "SOURCE: {{source_user}}\n",
    "\n",
    "CANDIDATES: \n",
    "{{candidate_users}}\n",
    "\n",
    "LLM OUTPUT:\n",
    "{{llm_output}}\n",
    "\"\"\".format(\n",
    "    label_list=\", \".join([f'\"{label}\"' for label in LABEL_SCORE_MAP.keys()]),\n",
    "    output_schema=\"\"\"\n",
    "{{\n",
    "    \"correct\": bool,\n",
    "    \"explanation\": str,\n",
    "    \"new_ranking\": {{\n",
    "        \"user_name_1\": label_1,\n",
    "        \"user_name_2\": label_2,\n",
    "        ...\n",
    "    }} | None\n",
    "}}\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_judge_prompt(source_user, candidate_users, llm_output):\n",
    "    return JUDGE_PROMPT.format(\n",
    "        source_user=source_user, candidate_users=candidate_users, llm_output=llm_output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pl.DataFrame(\n",
    "    schema={\n",
    "        \"source_user\": pl.Utf8,\n",
    "        \"ranked_candidates\": pl.Struct({\"author\": pl.Utf8, \"distance\": pl.Float64}),\n",
    "        \"llm_output\": pl.Utf8,\n",
    "        \"correct\": pl.Boolean,\n",
    "        \"new_ranking\": pl.Utf8,\n",
    "        \"explanation\": pl.Utf8,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_embeddings.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher output:  Okay, here's an assessment of the similarity between the SOURCE and each CANDIDATE, with labels as requested, based on the analysis of their Reddit activity.  This is a fairly in-depth analysis, trying to infer personality, interests, and overall \"vibe.\"  I'll also give a brief explanation for each rating.\n",
      "\n",
      "**Key:**\n",
      "\n",
      "*   **Most Similar:** Shares a *very* high degree of overlap in interests, personality, communication style, and likely worldview.\n",
      "*   **Highly Similar:**  Shares substantial overlap but with some discernible differences.\n",
      "*   **Somewhat Similar:**  Some common ground, but also noticeable differences. A casual acquaintance level of similarity.\n",
      "*   **Not Similar:**  Little to no discernible overlap.  Distinct interests and likely personality traits.\n",
      "\n",
      "---\n",
      "\n",
      "**Similarity Ratings:**\n",
      "\n",
      "*   **dQw4w9WgXc:** **Highly Similar** - This user exhibits a broad range of interests, some gaming, some general discussion, with a generally inquisitive and thoughtful approach.  Similar to the SOURCE's varied activity. The questioning nature of the posts and engagement in discussions shows a comparable curiosity. They are also all over the place like the source.\n",
      "*   **Blarma1:** **Somewhat Similar** - There's shared interest in the gaming sphere, specifically PC gaming. Similarity in starting a topic but quickly branching out into something else.\n",
      "*   **Buttlet:** **Somewhat Similar** - Focus on gaming (Dota 2), a somewhat casual and conversational tone like the SOURCE. However, the focus is narrower and has a clear leaning towards intense competitive gaming. Relatively calm and conversational.\n",
      "*   **ColonelRuffhouse:** **Not Similar** - This user is deeply engaged in political/social commentary, and more academic/thoughtful discussions.  A much broader, more serious/critical outlook than the SOURCE as evidenced by the content, making them notably different.\n",
      "*   **Coworker_as_Fuck:** **Highly Similar** - This user displays a similar kind of wide-ranging, sometimes cynical, but generally engaging style, evident across multiple subreddits. Shares a similar casual tone and a tendency to jump around between topics.\n",
      "*   **EightNation:** **Somewhat Similar** - Demonstrates a range of internet interests, from gaming (KSP) to asking general questions on Reddit. Shares the SOURCE's tendency for casual curiosity.\n",
      "*   **Ivanthe3rd:** **Highly Similar** - While focused on gaming and some technical aspects, the casual conversational tone and a willingness to explore different topics are very close to the SOURCE. The broader range of interests outside of the core gaming focus contributes to this similarity.\n",
      "*   **Sparkfairy:** **Somewhat Similar** - This user is a bit more focused on fashion, personal relationships, and concerns, but still displays a similar pattern of conversational posting and a willingness to discuss multiple topics, which is in common with the SOURCE.\n",
      "*   **deadrag3:** **Somewhat Similar** - Fairly active across multiple reddit topics, including, stark discord, and other technical interests. The range of topics they delve into is closer to the source than to other users.\n",
      "*    **jnthehnt:** **Somewhat Similar** - The user demonstrates a wide range of interests including various subjects with multiple communities across multiple subreddits. \n",
      "*   **lavender13:** **Not Similar** - This user has a very specific pattern of activity and focuses on one subreddit. The overall persona and posts come across as very different from the source.\n",
      "*    **piaband:** **Somewhat Similar** - An amalgam of various interests ranging from tags, roleplay and travels. They seem adventurous almost like the source.\n",
      "*   **ripleyclone8:** **Somewhat Similar** - Shows curiosity, a range of discussions, and a casual conversational style. Appears to be open to new ideas and have a generally friendly disposition.\n",
      "*   **sombreromaster:** **Somewhat Similar** - While active in specific communities, this user demonstrates a thoughtful and inquisitive approach, similar to the SOURCE. This user will ask what is occurring in more detail or discuss something more complexly.\n",
      "*   **spurios:** **Not Similar** - Explicitly political in ways that would not mesh with the source. \n",
      "*   **square965:** **Highly Similar** - This user shows a similar approach to browsing Reddit: engaging in diverse discussions, asking questions, offering help, and generally being interested in a wide variety of topics.\n",
      "*  **txBuilder**: **Somewhat Similar** -This user possesses a similar casual tone and storytelling ability as the source. They share a willingness to explore personal interests and travel plans, suggesting a common ground in seeking new experiences and self-discovery.\n",
      "*   **v66fender66v:** **Highly Similar** - This user is all over the place and is willing to discuss topics like remote troubleshooting, and guitar tech. \n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Reddit Persona vs. Real Life:** It's crucial to remember that online personas don't always accurately reflect reality.\n",
      "*   **Sample Size:** This analysis is based on the activity provided. A larger sample might yield different results.\n",
      "*   **Subjectivity:** Some degree of subjectivity is unavoidable in personality assessments.\n",
      "\n",
      "\n",
      "\n",
      "I hope this detailed breakdown is helpful! Let me know if you would like a more refined assessment of any specific pairing or further elaboration on a particular user.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [04:04, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[156]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTeacher output: \u001b[39m\u001b[33m\"\u001b[39m, teacher_output)\n\u001b[32m     16\u001b[39m teacher_output_json = repair_json(teacher_output, return_objects=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m judge_output = \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJUDGE_MODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_judge_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_users_formatted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_output_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mJudge output: \u001b[39m\u001b[33m\"\u001b[39m, judge_output)\n\u001b[32m     19\u001b[39m judge_output_json = repair_json(judge_output, return_objects=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[143]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mget_completion\u001b[39m\u001b[34m(model, data)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_completion\u001b[39m(model, data) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     13\u001b[39m     completion = openrouter_client.chat.completions.create(\n\u001b[32m     14\u001b[39m       extra_body={},\n\u001b[32m     15\u001b[39m       model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m       ]\n\u001b[32m     27\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.message.content\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "for row in tqdm(df_with_embeddings.iter_rows(named=True)):\n",
    "    current_user = row[\"author\"]\n",
    "    query_vector_2d = np.array(row[\"embedding\"], dtype=\"float32\").reshape(1, -1)\n",
    "    distances, indices = search_index(query_vector_2d)\n",
    "\n",
    "    candidate_users_df = df_with_embeddings.join(\n",
    "        pl.DataFrame({\"row_idx\": indices, \"distance\": distances}),\n",
    "        on=\"row_idx\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    candidate_users_formatted = \"\"\n",
    "    for candidate in candidate_users_df.iter_rows(named=True):\n",
    "        candidate_users_formatted += (\n",
    "            f\"User '{candidate['author']}' said: {candidate['formatted_comment']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    teacher_output = get_completion(\n",
    "        TEACHER_MODEL,\n",
    "        get_teacher_prompt(row[\"formatted_comment\"], candidate_users_formatted),\n",
    "    )\n",
    "    print(\"Teacher output: \", teacher_output)\n",
    "    teacher_output_json = repair_json(teacher_output, return_objects=True)\n",
    "    judge_output = get_completion(\n",
    "        JUDGE_MODEL,\n",
    "        get_judge_prompt(current_user, candidate_users_formatted, teacher_output_json),\n",
    "    )\n",
    "    print(\"Judge output: \", judge_output)\n",
    "    judge_output_json = repair_json(judge_output, return_objects=True)\n",
    "    # add row to results_df\n",
    "    results_df.extend(\n",
    "        pl.DataFrame(\n",
    "            {\n",
    "                \"source_user\": current_user,\n",
    "                \"ranked_candidates\": candidate_users_df.sort(\n",
    "                    \"distance\", descending=True\n",
    "                )\n",
    "                .select(\"author\", \"distance\")\n",
    "                .to_dicts(),\n",
    "                \"llm_output\": json.dumps(teacher_output_json),\n",
    "                \"correct\": judge_output_json[\"correct\"],\n",
    "                \"new_ranking\": json.dumps(judge_output_json[\"new_ranking\"]),\n",
    "                \"explanation\": judge_output_json[\"explanation\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    print(\"In progress: \", row[\"row_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
