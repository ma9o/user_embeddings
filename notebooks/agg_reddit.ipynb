{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Define constants\n",
    "DATA_PATH = \"../data/reddit/comments/RC_2015-01_parsed.ndjson\"\n",
    "TOKENS_PER_USER = 1000\n",
    "MIN_COMMENTS_THRESHOLD = 20\n",
    "SAMPLE_USER_COUNT = 100 # How many users to process (adjust as needed)\n",
    "HEIGHT = 53851542\n",
    "\n",
    "# 1. Load the parsed data\n",
    "df = pl.scan_ndjson(DATA_PATH, infer_schema_length=10000, schema_overrides={\"created_utc\": pl.Datetime})\n",
    "#height = len(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 2. Calculate user comment counts\n",
    "user_stats = df.group_by(\"author\").agg(\n",
    "    pl.len().alias(\"comment_count\"),\n",
    "    pl.sum(\"token_count_approx\").alias(\"total_tokens_approx\")\n",
    ")\n",
    "\n",
    "# 3. Filter out unwanted users\n",
    "filtered_users = user_stats.filter(\n",
    "    ~pl.col(\"author\").is_in([\"[deleted]\", \"AutoModerator\"])\n",
    ").filter(\n",
    "    ~pl.col(\"author\").str.contains(\"(?i)bot\") # Case-insensitive bot filter\n",
    ")\n",
    "\n",
    "# 4. Filter for users meeting the minimum comment threshold\n",
    "active_users = filtered_users.filter(\n",
    "    pl.col(\"comment_count\") >= MIN_COMMENTS_THRESHOLD\n",
    ").collect()\n",
    "\n",
    "# 5. Select a sample of users to process\n",
    "selected_user_sample = active_users.sample(n=SAMPLE_USER_COUNT).select(\"author\")\n",
    "\n",
    "# Collect the authors list for filtering the main dataframe\n",
    "selected_authors = selected_user_sample.get_column(\"author\").to_list()\n",
    "\n",
    "# 6. Filter the original DataFrame for selected authors' comments\n",
    "user_comments_lazy = df.filter(pl.col(\"author\").is_in(selected_authors))\n",
    "\n",
    "user_comments_shuffled = user_comments_lazy.with_columns(\n",
    "    # Generate a random float for each row within the author group\n",
    "    _random_sort_key=pl.lit(np.random.rand()).over(\"author\")\n",
    ").sort([\"author\", \"_random_sort_key\"]).drop(\"_random_sort_key\")\n",
    "\n",
    "\n",
    "# 7. Calculate cumulative tokens for each user (original order)\n",
    "#    The order within each author group is arbitrary at this stage (depends on scan order)\n",
    "user_comments_with_cumsum = user_comments_lazy.with_columns(\n",
    "    pl.col(\"token_count_approx\").cum_sum().over(\"author\").alias(\"cumulative_tokens\")\n",
    ")\n",
    "\n",
    "# 8. Filter comments to stay within the token limit for each user (based on original order)\n",
    "comments_within_limit = user_comments_with_cumsum.filter(\n",
    "    pl.col(\"cumulative_tokens\") <= TOKENS_PER_USER\n",
    ")\n",
    "\n",
    "# 9. Sort the *selected* comments chronologically for each user\n",
    "#    Do this *after* filtering by token limit\n",
    "comments_sorted_for_formatting = comments_within_limit.sort([\"author\", \"created_utc\"])\n",
    "\n",
    "# 10. Format the selected and now sorted comments with the date\n",
    "formatted_comments = comments_sorted_for_formatting.with_columns(\n",
    "    formatted_comment=pl.format(\n",
    "        \"r/{}: {}\",\n",
    "        pl.col(\"subreddit\"),\n",
    "        #pl.col(\"created_utc\").dt.date(),\n",
    "        pl.col(\"body\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 11. Aggregate the formatted, token-limited comments for each user\n",
    "final_user_texts = formatted_comments.group_by(\"author\").agg(\n",
    "    pl.col(\"formatted_comment\").str.join(\"\\n\") # Join comments with double newline\n",
    ").select(\"author\", \"formatted_comment\") # Reorder columns\n",
    "\n",
    "# Collect the results\n",
    "final_df = final_user_texts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>author</th><th>formatted_comment</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;Archion&quot;</td><td>&quot;r/pitbulls: So it&#x27;s not just m…</td></tr><tr><td>&quot;Blarma1&quot;</td><td>&quot;r/KerbalSpaceProgram: Winning?…</td></tr><tr><td>&quot;Buttlet&quot;</td><td>&quot;r/DotA2: It&#x27;s not, it&#x27;s a net …</td></tr><tr><td>&quot;ColonelRuffhouse&quot;</td><td>&quot;r/wallpapers: 2014 was decided…</td></tr><tr><td>&quot;ComoTeLamas&quot;</td><td>&quot;r/FIFA: Yeah I&#x27;ve been wonderi…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌──────────────────┬─────────────────────────────────┐\n",
       "│ author           ┆ formatted_comment               │\n",
       "│ ---              ┆ ---                             │\n",
       "│ str              ┆ str                             │\n",
       "╞══════════════════╪═════════════════════════════════╡\n",
       "│ Archion          ┆ r/pitbulls: So it's not just m… │\n",
       "│ Blarma1          ┆ r/KerbalSpaceProgram: Winning?… │\n",
       "│ Buttlet          ┆ r/DotA2: It's not, it's a net … │\n",
       "│ ColonelRuffhouse ┆ r/wallpapers: 2014 was decided… │\n",
       "│ ComoTeLamas      ┆ r/FIFA: Yeah I've been wonderi… │\n",
       "└──────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write_ndjson(\"../data/reddit/comments/RC_2015-01_prepared.ndjson\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
