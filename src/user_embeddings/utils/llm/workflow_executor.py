import asyncio
import logging
from typing import Any, Callable, Dict, List, Optional, Tuple, TypedDict

import polars as pl
from pydantic import BaseModel, ValidationError
from tqdm.asyncio import tqdm_asyncio

from user_embeddings.utils.parsing import parse_llm_json_output

# Assuming get_text_completion is available via relative import or PYTHONPATH
# Adjust the import path based on your project structure if necessary
from .get_text_completion import get_text_completion

logger = logging.getLogger(__name__)


# --- Type Definitions ---
class TaskDefinition(TypedDict):
    """Defines a single task within a workflow stage."""

    prompt: str  # Name of the prompt module (also serves as task ID)
    input_from: Optional[
        List[str]
    ]  # List of task IDs from PREVIOUS stages or "__RAW_INPUT__"


class WorkflowStage(TypedDict):
    """Defines the structure for a single stage in a workflow."""

    stage: int
    tasks: List[TaskDefinition]  # Tasks to run in this stage


# --- Constants ---
# Removed FINAL_MERGED_OUTPUT_KEY as merging is no longer done explicitly here
# The output of a task is its validated data or an error indicator.
RAW_INPUT_PLACEHOLDER = "__RAW_INPUT__"  # Special key to inject initial workflow input


# --- Workflow Validation ---
def validate_workflow(
    workflow_name: str,
    workflow_definition: List[WorkflowStage],  # Updated type hint
    available_prompts: Dict[str, Tuple[str, str]],
    # Removed available_formatters
    available_output_models: Dict[
        str, type[BaseModel]
    ],  # Added for checking parsable tasks
    # available_input_formatters: Optional[Dict[str, Callable[[Dict[str, Any]], str]]] = None # Optional validation
) -> bool:
    """
    Validates a workflow definition against available prompts and output models.

    Checks for:
        - Existence of all specified prompt modules.
        - If a prompt module has a corresponding Pydantic model, it's considered parsable.
        - Correct dependencies (input_from tasks must exist in previous stages,
          unless it's the special placeholder RAW_INPUT_PLACEHOLDER).

    Returns:
        True if the workflow is valid, False otherwise. Prints errors to console.
    """
    is_valid = True
    defined_outputs = (
        set()
    )  # Keep track of outputs (task IDs) generated by previous stages

    # Sort stages first to ensure correct dependency checking
    sorted_workflow = sorted(workflow_definition, key=lambda x: x["stage"])
    stage_numbers = {s["stage"] for s in sorted_workflow}
    if len(stage_numbers) != len(sorted_workflow) or sorted(stage_numbers) != list(
        range(1, len(sorted_workflow) + 1)
    ):
        logger.error(
            f"Workflow '{workflow_name}' has duplicate or non-sequential stage numbers: {sorted(stage_numbers)}"
        )
        is_valid = False

    all_task_ids_in_workflow = set()
    tasks_per_stage: Dict[int, List[str]] = {}

    for stage_def in sorted_workflow:
        stage_num = stage_def["stage"]
        current_stage_tasks = []

        if "tasks" not in stage_def or not stage_def["tasks"]:
            logger.error(
                f"Stage {stage_num} in workflow '{workflow_name}' has no tasks defined."
            )
            is_valid = False
            continue  # Skip further checks for this empty stage

        for task_def in stage_def["tasks"]:
            task_id = task_def.get("prompt")
            if not task_id:
                logger.error(
                    f"A task in stage {stage_num} of workflow '{workflow_name}' is missing the 'prompt' key."
                )
                is_valid = False
                continue

            current_stage_tasks.append(task_id)

            # Check for duplicate task IDs within the same stage (implicitly handled by later check if needed)
            if task_id in all_task_ids_in_workflow:
                logger.error(
                    f"Task ID '{task_id}' is defined multiple times in workflow '{workflow_name}'. Task IDs must be unique across all stages."
                )
                is_valid = False

            all_task_ids_in_workflow.add(task_id)

            # Validate prompt existence
            if task_id not in available_prompts:
                logger.error(
                    f"Prompt module '{task_id}' for task in stage {stage_num} "
                    f"of workflow '{workflow_name}' not found. "
                    f"Available prompts: {list(available_prompts.keys())}"
                )
                is_valid = False

            # Validate input_from dependencies
            input_from_tasks = task_def.get("input_from")
            if input_from_tasks:
                # Allow RAW_INPUT_PLACEHOLDER in stage 1, but warn if other inputs are specified unnecessarily
                if stage_num == 1 and any(
                    t != RAW_INPUT_PLACEHOLDER for t in input_from_tasks
                ):
                    logger.warning(
                        f"Task '{task_id}' in stage 1 of workflow '{workflow_name}' specifies 'input_from' other than '{RAW_INPUT_PLACEHOLDER}'. This is unusual for the first stage."
                    )
                for required_input_task in input_from_tasks:
                    # Skip check if it's the raw input placeholder
                    if required_input_task == RAW_INPUT_PLACEHOLDER:
                        continue
                    # Check if the required non-raw input task exists in previous stages
                    if required_input_task not in defined_outputs:
                        logger.error(
                            f"Task '{task_id}' in stage {stage_num} requires input from '{required_input_task}', "
                            f"but it's not generated by any previous stage in workflow '{workflow_name}'. "
                            f"Available previous outputs: {list(defined_outputs)}"
                        )
                        is_valid = False
                    # Check if the required input task is actually parsable if this task expects structured input
                    # This check might be too strict, maybe a task just needs the raw string.
                    # We'll rely on runtime checks for now.

        tasks_per_stage[stage_num] = current_stage_tasks
        # Add outputs of this stage to the set for next stage validation
        defined_outputs.update(current_stage_tasks)

    return is_valid


# --- Core Execution Logic ---
async def _run_single_prompt(model_name: str, prompt: str) -> str:
    """Helper to run a single model prompt and handle errors."""
    try:
        # This assumes get_text_completion is initialized elsewhere or handles it
        result = await get_text_completion(model_name, prompt)
        return result
    except Exception as e:
        logger.error(f"Error running model {model_name} for prompt: {e}")
        # Return a standardized error string
        return f"ERROR: Model execution failed - {e}"


async def execute_workflow(
    model_name: str,
    initial_input: str,
    workflow: List[WorkflowStage],  # Updated type hint
    available_prompts: Dict[str, Tuple[str, str]],  # Map: name -> (text, version)
    available_output_models: Dict[str, type[BaseModel]],  # Added for parsing
    available_input_formatters: Optional[
        Dict[str, Callable[[Dict[str, Any]], str]]
    ] = None,  # Added input formatters
) -> Dict[str, Any]:
    """Executes a defined workflow for a single model and initial input.
    Parses and validates the output of each task using corresponding Pydantic models
    if available in `available_output_models`. Allows injecting the `initial_input`
    at any stage using the `RAW_INPUT_PLACEHOLDER` ("__RAW_INPUT__") in `input_from`.
    Input for each task is determined solely by its `input_from` definition.

    Args:
        model_name: The name of the LLM to use.
        initial_input: The starting input data for the workflow.
        workflow: List of stage definitions.
        available_prompts: Mapping of prompt module names to (prompt_text, version).
        available_output_models: Mapping of prompt module names to Pydantic models.
        available_input_formatters: Optional mapping of prompt module names to functions
                                    that format the input dictionary into a string.
                                    If None or a task_id is missing, defaults to json.dumps.

    Returns:
        A dictionary containing mappings from task_id (prompt name) to its result.
        The result can be:
            - A validated Pydantic model instance if parsing/validation succeeded.
            - The raw string output if no Pydantic model was defined.
            - An error string (e.g., "ERROR: Parse Failed", "ERROR: Validation Failed")
              if parsing/validation was attempted but failed.
            - An error string (e.g., "ERROR: Model execution failed", "ERROR: Missing required inputs")
              if the task couldn't run.
            - The `initial_input` string if requested via `RAW_INPUT_PLACEHOLDER`.
    """
    intermediate_results: Dict[str, Any] = {}  # Stores validated outputs or errors
    raw_outputs: Dict[str, str] = {}  # Stores raw LLM string outputs

    sorted_workflow = sorted(
        workflow, key=lambda x: x["stage"]
    )  # Ensure stages run in order

    for stage_def in sorted_workflow:
        stage_num = stage_def["stage"]
        tasks_in_stage: List[TaskDefinition] = stage_def["tasks"]

        # --- Prepare and run tasks concurrently within the stage ---
        stage_tasks_coroutines = []
        tasks_to_run_metadata: List[Dict[str, Any]] = []

        for task_def in tasks_in_stage:
            task_id = task_def["prompt"]
            input_from_tasks: Optional[List[str]] = task_def.get("input_from")
            current_task_input_str = ""  # Default to empty, determined by input_from
            skip_task = False
            error_msg = None

            # Always check if raw input is requested first
            raw_input_requested = (
                input_from_tasks and RAW_INPUT_PLACEHOLDER in input_from_tasks
            )
            if raw_input_requested:
                inputs_to_serialize = {}
                inputs_to_serialize[RAW_INPUT_PLACEHOLDER] = initial_input

            # Determine other dependencies
            other_input_tasks = []
            if input_from_tasks:
                other_input_tasks = [
                    t for t in input_from_tasks if t != RAW_INPUT_PLACEHOLDER
                ]

            if not other_input_tasks:
                # Case 1: No dependencies OR only raw input requested
                if not input_from_tasks:  # No input_from specified at all
                    # This implies the task needs no input.
                    # If a task needs input, it must specify it via `input_from`.
                    current_task_input_str = ""  # Task takes no input data
                elif raw_input_requested:  # Only raw input requested
                    current_task_input_str = initial_input
                # else: input_from_tasks was empty or None - handled above
            else:
                # Case 2: Depends on previous tasks (and potentially raw input)
                all_inputs_available = True
                for req_task_id in other_input_tasks:
                    if req_task_id not in intermediate_results:
                        all_inputs_available = False
                        error_msg = f"ERROR: Missing required input '{req_task_id}' for task '{task_id}' in stage {stage_num} (model {model_name}). Available: {list(intermediate_results.keys())}"
                        break
                    input_val = intermediate_results[req_task_id]
                    if isinstance(input_val, str) and input_val.startswith("ERROR:"):
                        all_inputs_available = False
                        error_msg = f"ERROR: Required input '{req_task_id}' for task '{task_id}' failed in a previous step: {input_val}"
                        break
                    inputs_to_serialize[req_task_id] = input_val

                if not all_inputs_available:
                    logger.error(error_msg)
                    skip_task = True
                elif not inputs_to_serialize:  # Should not happen if other_input_tasks was non-empty and all_inputs_available
                    error_msg = f"ERROR: Internal logic error - failed to collect any valid inputs for task '{task_id}' stage {stage_num}."
                    logger.error(error_msg)
                    skip_task = True
                else:
                    # Serialize the collected inputs (could include RAW_INPUT_PLACEHOLDER key)
                    # --- Input Formatting ---
                    formatter = (
                        available_input_formatters.get(task_id)
                        if available_input_formatters
                        else None
                    )
                    if formatter:
                        # 1. Use the custom formatter if provided
                        try:
                            current_task_input_str = formatter(inputs_to_serialize)
                        except Exception as e:
                            error_msg = f"ERROR: Custom input formatter for task '{task_id}' failed: {e}"
                            logger.error(error_msg)
                            skip_task = True
                    else:
                        # 2. No custom formatter provided
                        num_inputs = len(inputs_to_serialize)
                        if num_inputs == 0:
                            # Task takes no input
                            current_task_input_str = ""
                        elif num_inputs == 1:
                            # Task takes exactly one input
                            try:
                                # Get the single value (key doesn't matter)
                                single_input_value = next(
                                    iter(inputs_to_serialize.values())
                                )
                                if isinstance(single_input_value, BaseModel):
                                    # Serialize Pydantic model to compact JSON
                                    current_task_input_str = (
                                        single_input_value.model_dump_json(indent=None)
                                    )
                                else:
                                    # Use the string representation for other types
                                    current_task_input_str = str(single_input_value)
                            except Exception as e:
                                error_msg = f"ERROR: Failed to process single input for task '{task_id}': {e}"
                                logger.error(error_msg)
                                skip_task = True
                        else:  # num_inputs > 1
                            # Task takes multiple inputs, but no formatter defined - Ambiguous!
                            error_msg = f"ERROR: Ambiguous Input Formatting - Task '{task_id}' requires {num_inputs} inputs but no specific formatter is defined."
                            logger.error(error_msg)
                            skip_task = True

            # 2. Create task coroutine if not skipped
            if skip_task:
                intermediate_results[task_id] = error_msg
                raw_outputs[task_id] = error_msg  # Also store error in raw
            else:
                try:
                    instruction_prompt_text = available_prompts[task_id][0]
                    model_prompt = (
                        f"{instruction_prompt_text}\n\nINPUT DATA:\n---\n"
                        f"{current_task_input_str}\n---"
                    )
                    stage_tasks_coroutines.append(
                        asyncio.create_task(
                            _run_single_prompt(model_name, model_prompt)
                        )
                    )
                    tasks_to_run_metadata.append({"task_id": task_id})
                except KeyError:
                    error_msg = f"ERROR: Prompt module name '{task_id}' not found in available_prompts."
                    logger.error(error_msg)
                    intermediate_results[task_id] = error_msg
                    raw_outputs[task_id] = error_msg
                except Exception as e:
                    error_msg = f"ERROR: Failed to create task coroutine for '{task_id}' in stage {stage_num}: {e}"
                    logger.error(error_msg)
                    intermediate_results[task_id] = error_msg
                    raw_outputs[task_id] = error_msg

        # 3. Gather raw results for the stage
        if stage_tasks_coroutines:
            stage_raw_results = await asyncio.gather(*stage_tasks_coroutines)
        else:
            stage_raw_results = []

        # 4. Process raw results: store raw output and attempt parsing/validation
        for i, raw_result_str in enumerate(stage_raw_results):
            task_id = tasks_to_run_metadata[i]["task_id"]
            raw_outputs[task_id] = raw_result_str  # Always store the raw output

            if raw_result_str.startswith("ERROR:"):
                intermediate_results[task_id] = raw_result_str  # Propagate error
                logger.error(
                    f"Task '{task_id}' in stage {stage_num} failed execution for model {model_name}."
                )
                continue

            # Attempt parsing/validation only if a Pydantic model is defined
            output_model = available_output_models.get(task_id)
            if output_model:
                parsed_dict = parse_llm_json_output(raw_result_str, expect_type=dict)
                if parsed_dict is None:
                    intermediate_results[task_id] = "ERROR: Parse Failed"
                    logger.warning(
                        f"Failed to parse JSON output for task '{task_id}', model '{model_name}'. Raw output stored."
                    )
                else:
                    try:
                        validated_data: BaseModel = output_model.model_validate(
                            parsed_dict
                        )
                        intermediate_results[task_id] = (
                            validated_data  # Store the validated Pydantic object
                        )
                        # print(f"Task '{task_id}' output validated successfully.") # Debug log
                    except ValidationError as ve:
                        intermediate_results[task_id] = (
                            f"ERROR: Validation Failed - {ve}"
                        )
                        logger.warning(
                            f"Pydantic validation failed for task '{task_id}', model '{model_name}': {ve}. Raw output stored."
                        )
            else:
                # No model defined, store the raw string output as the result
                intermediate_results[task_id] = raw_result_str
                # print(f"Task '{task_id}' has no Pydantic model defined, storing raw output.") # Debug log

    # Return all results (validated models, raw strings, or errors)
    # Add raw outputs for debugging or potential fallback needs downstream
    return {"validated_outputs": intermediate_results, "raw_outputs": raw_outputs}


# Updated FUNCTION to orchestrate runs over samples and models
async def run_workflow_on_samples(
    sample_df: pl.DataFrame,
    models_to_test: List[str],
    workflow: List[WorkflowStage],  # Updated type hint
    available_prompts: Dict[str, Tuple[str, str]],  # Map: name -> (text, version)
    available_output_models: Dict[str, type[BaseModel]],  # Pass models for validation
    input_column: str = "formatted_context",  # Allow specifying input column
    available_input_formatters: Optional[
        Dict[str, Callable[[Dict[str, Any]], str]]
    ] = None,  # Added
) -> List[Dict[str, Any]]:
    """
    Runs a defined workflow concurrently across multiple samples and models.
    Uses execute_workflow which handles parsing and validation internally.

    Args:
        sample_df: Polars DataFrame with input data.
        models_to_test: List of model names to run.
        workflow: The workflow definition.
        available_prompts: Map of prompt names to (prompt_text, version).
        available_output_models: Map of prompt names to Pydantic models.
        input_column: The column name in sample_df containing the initial input.
        available_input_formatters: Optional map of prompt names to input formatting functions.

    Returns:
        A list of dictionaries, one per sample. Each dictionary contains:
            'input_data': The initial input for the sample.
            'model_outputs': A dict mapping model_name -> execution_result, where
                             execution_result contains:
                                - 'validated_outputs': {task_id: validated_pydantic_object | raw_string | error_string}
                                - 'raw_outputs': {task_id: raw_llm_output_string | error_string}
    """
    all_tasks = []
    task_metadata = []  # Stores sample_index, model, initial_input
    logger.info("Preparing workflow tasks for all samples and models...")

    if input_column not in sample_df.columns:
        raise ValueError(f"Input column '{input_column}' not found in DataFrame.")

    for i, row in enumerate(sample_df.iter_rows(named=True)):
        initial_input_data = row[input_column]
        for model in models_to_test:
            task = asyncio.create_task(
                execute_workflow(  # Use the single-sample executor
                    model_name=model,
                    initial_input=initial_input_data,
                    workflow=workflow,
                    available_prompts=available_prompts,
                    available_output_models=available_output_models,  # Pass models
                    available_input_formatters=available_input_formatters,  # Pass formatters
                )
            )
            all_tasks.append(task)
            task_metadata.append(
                {
                    "sample_index": i,
                    "model": model,
                    "initial_input_data": initial_input_data,
                }
            )

    logger.info(f"Running {len(all_tasks)} workflow tasks concurrently...")
    # Each result is a dict: {"validated_outputs": {...}, "raw_outputs": {...}}
    all_individual_results: List[Dict[str, Any]] = await tqdm_asyncio.gather(
        *all_tasks, desc="Running Test Model Workflows", unit="task"
    )

    # --- Organize results by sample index ---
    logger.info("Organizing workflow results by sample...")
    # results_by_sample will store structured results per sample
    results_by_sample: List[Dict[str, Any]] = [
        {"input_data": None, "model_outputs": {}} for _ in range(len(sample_df))
    ]

    for i, execution_result_dict in enumerate(all_individual_results):
        meta = task_metadata[i]
        sample_index = meta["sample_index"]
        model = meta["model"]

        # Initialize sample dict if first time seeing this index
        if results_by_sample[sample_index]["input_data"] is None:
            results_by_sample[sample_index]["input_data"] = meta["initial_input_data"]

        # Store the entire result dictionary (containing validated and raw outputs)
        # for this model under the 'model_outputs' key for the sample.
        results_by_sample[sample_index]["model_outputs"][model] = execution_result_dict

    return results_by_sample
