import asyncio
import json
import re
from typing import Any, Callable, Dict, List, Optional, TypedDict

import polars as pl
from json_repair import repair_json
from tqdm.asyncio import tqdm_asyncio

# Assuming get_text_completion is available via relative import or PYTHONPATH
# Adjust the import path based on your project structure if necessary
from .get_text_completion import get_text_completion


# --- Type Definitions ---
class PromptStage(TypedDict):
    """Defines the structure for a single stage in a workflow."""

    stage: int
    prompts: List[str]  # List of prompt module names (used as task IDs)
    input_from: Optional[List[str]]  # List of task IDs from previous stage(s)
    input_formatter: Optional[str]  # Name of formatter function


# --- Constants ---
# Key used to store the automatically merged JSON output from the final stage
FINAL_MERGED_OUTPUT_KEY = "_final_merged_output"


# --- Input Formatters ---
def format_single_input(outputs: Dict[str, str]) -> str:
    """Default formatter for single input stages."""
    if len(outputs) != 1:
        raise ValueError(
            f"Expected exactly one input for format_single_input, got {len(outputs)}"
        )
    return list(outputs.values())[0]


def format_parallel_outputs(outputs: Dict[str, str]) -> str:
    """Formatter for combining multiple parallel outputs."""
    formatted = "Combined inputs:\n"
    # Sort by task_id (prompt name) for consistent order
    for task_id, output in sorted(outputs.items()):
        formatted += f"--- Output from {task_id} ---\n{output}\n"
    return formatted.strip()


# Default available input formatters
DEFAULT_INPUT_FORMATTERS: Dict[str, Callable[[Dict[str, str]], str]] = {
    "format_single_input": format_single_input,
    "format_parallel_outputs": format_parallel_outputs,
}


# --- Workflow Validation ---
def validate_workflow(
    workflow_name: str,
    workflow_definition: List[PromptStage],
    available_prompts: Dict[str, str],
    available_formatters: Dict[str, Callable],
) -> bool:
    """
    Validates a workflow definition against available prompts and formatters.

    Checks for:
        - Existence of all specified prompt modules.
        - Existence of all specified input formatters.
        - Correct dependencies (input_from tasks must exist in previous stages).

    Returns:
        True if the workflow is valid, False otherwise. Prints errors to console.
    """
    is_valid = True
    defined_outputs = set()  # Keep track of outputs generated by previous stages

    # Sort stages first to ensure correct dependency checking
    sorted_workflow = sorted(workflow_definition, key=lambda x: x["stage"])

    for stage_def in sorted_workflow:
        stage_num = stage_def["stage"]

        # Validate prompts list
        for prompt_module_name in stage_def["prompts"]:
            if prompt_module_name not in available_prompts:
                print(
                    f"Error: Prompt module '{prompt_module_name}' in stage {stage_num} "
                    f"of workflow '{workflow_name}' not found. "
                    f"Available prompts: {list(available_prompts.keys())}"
                )
                is_valid = False

        # Validate formatter if specified
        formatter_name = stage_def.get("input_formatter")
        if formatter_name and formatter_name not in available_formatters:
            print(
                f"Error: Input formatter '{formatter_name}' in stage {stage_num} "
                f"of workflow '{workflow_name}' not found. "
                f"Available formatters: {list(available_formatters.keys())}"
            )
            is_valid = False

        # Validate input_from dependencies (only for stages > 1)
        if stage_num > 1:
            input_from_tasks = stage_def.get("input_from")
            if not input_from_tasks:
                # Check if previous stage has exactly one output, otherwise requires input_from
                prev_stage_num = stage_num - 1
                prev_stage_prompts = []
                for prev_stage in sorted_workflow:
                    if prev_stage["stage"] == prev_stage_num:
                        prev_stage_prompts = prev_stage["prompts"]
                        break
                if len(prev_stage_prompts) != 1:
                    print(
                        f"Error: Stage {stage_num} in workflow '{workflow_name}' implicitly requires a single input "
                        f"from stage {prev_stage_num}, but that stage defines {len(prev_stage_prompts)} prompts. "
                        f"Explicitly define 'input_from' for stage {stage_num}."
                    )
                    is_valid = False

            elif input_from_tasks:  # If input_from *is* specified
                for required_input_task in input_from_tasks:
                    if required_input_task not in defined_outputs:
                        print(
                            f"Error: Stage {stage_num} requires input from '{required_input_task}', "
                            f"but it's not generated by any previous stage in workflow '{workflow_name}'. "
                            f"Available previous outputs: {list(defined_outputs)}"
                        )
                        is_valid = False

        # Add outputs of this stage to the set for next stage validation
        defined_outputs.update(stage_def["prompts"])

    return is_valid


# --- Core Execution Logic ---
async def _run_single_prompt(model_name: str, prompt: str) -> str:
    """Helper to run a single model prompt and handle errors."""
    try:
        # This assumes get_text_completion is initialized elsewhere or handles it
        result = await get_text_completion(model_name, prompt)
        return result
    except Exception as e:
        print(f"Error running model {model_name} for prompt: {e}")
        # Return a standardized error string
        return f"ERROR: Model execution failed - {e}"


async def execute_workflow(
    model_name: str,
    initial_input: str,
    workflow: List[PromptStage],  # Use the defined type
    available_prompts: Dict[str, str],  # Map: prompt_module_name -> prompt_text
    input_formatters: Dict[str, Callable[[Dict[str, str]], str]],
) -> Dict[str, Any]:
    """Executes a defined workflow for a single model and initial input.
    If the final stage involves multiple parallel prompts that all successfully
    return valid JSON, their outputs will be automatically parsed and merged
    into a single dictionary stored under the key '_final_merged_output'.

    Args:
        model_name: The name of the LLM to use.
        initial_input: The starting input data for the first stage.
        workflow: List of stage definitions.
        available_prompts: A dictionary mapping prompt module names to actual prompt text.
        input_formatters: A dictionary mapping formatter names to callable functions.

    Returns:
        A dictionary containing:
            - Mappings from prompt_module_name (task_id) to its raw string output.
            - Optionally, the key '_final_merged_output' mapping to a dictionary
              if automatic merging of the final stage's JSON outputs was successful.
    """
    intermediate_results: Dict[str, Any] = {}
    current_stage_input = initial_input
    sorted_workflow = sorted(
        workflow, key=lambda x: x["stage"]
    )  # Ensure stages run in order

    for stage_def in sorted_workflow:
        stage_num = stage_def["stage"]
        prompts_in_stage: List[str] = stage_def["prompts"]
        input_from_tasks: Optional[List[str]] = stage_def.get("input_from")
        formatter_name = stage_def.get("input_formatter")

        # 1. Prepare input for the current stage
        if stage_num > 1:
            if not input_from_tasks:
                # Find outputs from the immediate previous stage
                prev_stage_outputs = {}
                for prev_stage in sorted_workflow:
                    if prev_stage["stage"] == stage_num - 1:
                        for task_id in prev_stage["prompts"]:
                            # Check if previous result was a string (could be merged dict later)
                            if task_id in intermediate_results and isinstance(
                                intermediate_results[task_id], str
                            ):
                                prev_stage_outputs[task_id] = intermediate_results[
                                    task_id
                                ]
                        break  # Found previous stage

                if len(prev_stage_outputs) == 1:
                    current_stage_input = list(prev_stage_outputs.values())[0]
                    print(
                        f"Warning: Stage {stage_num} model {model_name} using single output from stage {stage_num - 1} as input due to missing 'input_from'."
                    )
                elif len(prev_stage_outputs) != 1 and stage_num > 1:
                    error_msg = (
                        f"ERROR: Ambiguous input for stage {stage_num} model {model_name}. "
                        f"Need 'input_from' when previous stage has multiple/no/failed outputs ({len(prev_stage_outputs)} valid string outputs found)."
                    )
                    print(error_msg)
                    for prompt_module_name in prompts_in_stage:
                        intermediate_results[prompt_module_name] = error_msg
                    break

            else:  # input_from_tasks is specified
                try:
                    # Gather required inputs (ensure they are strings)
                    inputs_to_format = {}
                    all_inputs_valid = True
                    for task_id in input_from_tasks:
                        input_val = intermediate_results.get(task_id)
                        if isinstance(input_val, str) and not input_val.startswith(
                            "ERROR:"
                        ):
                            inputs_to_format[task_id] = input_val
                        else:
                            all_inputs_valid = False
                            break  # Missing or non-string input

                    if not all_inputs_valid or len(inputs_to_format) != len(
                        input_from_tasks
                    ):
                        missing_tasks = set(input_from_tasks) - set(
                            inputs_to_format.keys()
                        )
                        error_msg = (
                            f"ERROR: Missing/invalid required string inputs {list(missing_tasks)} for stage {stage_num} "
                            f"model {model_name} due to previous errors or non-string outputs."
                        )
                        print(error_msg)
                        for prompt_module_name in prompts_in_stage:
                            intermediate_results[prompt_module_name] = error_msg
                        break  # Stop processing

                    # Apply input formatter (expects Dict[str, str])
                    if not formatter_name:
                        if len(inputs_to_format) == 1:
                            current_stage_input = list(inputs_to_format.values())[0]
                        else:
                            error_msg = (
                                f"ERROR: Stage {stage_num} model {model_name} needs multiple inputs "
                                f"({input_from_tasks}) but no 'input_formatter' specified."
                            )
                            print(error_msg)
                            for prompt_module_name in prompts_in_stage:
                                intermediate_results[prompt_module_name] = error_msg
                            break
                    elif formatter_name not in input_formatters:
                        error_msg = f"ERROR: Specified input_formatter '{formatter_name}' not found."
                        print(error_msg)
                        for prompt_module_name in prompts_in_stage:
                            intermediate_results[prompt_module_name] = error_msg
                        break
                    else:
                        formatter_func = input_formatters[formatter_name]
                        current_stage_input = formatter_func(inputs_to_format)

                except Exception as e:
                    error_msg = f"ERROR: Failed to format inputs for stage {stage_num} model {model_name}: {e}"
                    print(error_msg)
                    for prompt_module_name in prompts_in_stage:
                        intermediate_results[prompt_module_name] = error_msg
                    break

        # 2. Run prompts in the current stage concurrently
        stage_tasks = []
        tasks_to_run_ids: List[str] = []
        for prompt_module_name in prompts_in_stage:
            try:
                instruction_prompt = available_prompts[prompt_module_name]
                # Ensure current_stage_input is a string before formatting prompt
                if not isinstance(current_stage_input, str):
                    raise TypeError(f"Input for stage {stage_num} is not a string.")

                model_prompt = (
                    f"{instruction_prompt}\n\nINPUT DATA:\n---\n"
                    f"{current_stage_input}\n---"
                )
                stage_tasks.append(
                    asyncio.create_task(_run_single_prompt(model_name, model_prompt))
                )
                tasks_to_run_ids.append(prompt_module_name)  # Use module name as ID
            except KeyError:
                error_msg = f"ERROR: Prompt module name '{prompt_module_name}' not found in available_prompts."
                print(error_msg)
                intermediate_results[prompt_module_name] = error_msg  # Mark as failed
            except Exception as e:
                error_msg = f"ERROR: Failed to create task for '{prompt_module_name}' in stage {stage_num}: {e}"
                print(error_msg)
                intermediate_results[prompt_module_name] = error_msg  # Mark as failed

        # 3. Gather results for the stage
        if stage_tasks:
            stage_results = await asyncio.gather(*stage_tasks)
        else:
            stage_results = []

        # 4. Store results using prompt_module_name as the key (still storing raw strings)
        for task_id, result in zip(tasks_to_run_ids, stage_results):
            intermediate_results[task_id] = result
            if result.startswith("ERROR:"):
                print(
                    f"Task '{task_id}' in stage {stage_num} failed for model {model_name}."
                )

    # --- Post-Workflow: Attempt Final Stage JSON Merging ---
    if sorted_workflow:  # Check if workflow is not empty
        final_stage_def = sorted_workflow[-1]
        final_stage_task_ids = final_stage_def["prompts"]

        if len(final_stage_task_ids) > 1:
            merged_data = {}
            can_merge = True
            parse_errors = []

            for task_id in final_stage_task_ids:
                raw_output = intermediate_results.get(task_id)
                if not isinstance(raw_output, str) or raw_output.startswith("ERROR:"):
                    can_merge = False
                    parse_errors.append(
                        f"Task '{task_id}': Output missing or is error."
                    )
                    break  # Cannot merge if any task failed

                # Attempt to parse JSON from the output string
                try:
                    # Basic JSON block extraction (like in judge parsing)
                    match = re.search(
                        r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", raw_output, re.DOTALL
                    )
                    if match:
                        json_str = match.group(1)
                    else:
                        # Try repairing if no markdown block found
                        json_str = repair_json(raw_output.strip())

                    # Attempt to parse the cleaned/repaired string
                    parsed_dict = json.loads(json_str)

                    if isinstance(parsed_dict, dict):
                        merged_data.update(parsed_dict)  # Merge the dictionaries
                    else:
                        can_merge = False
                        parse_errors.append(
                            f"Task '{task_id}': Parsed JSON is not a dictionary."
                        )
                        break

                except (json.JSONDecodeError, TypeError) as e:
                    can_merge = False
                    parse_errors.append(
                        f"Task '{task_id}': JSON parsing failed - {e}. Raw: {raw_output[:50]}..."
                    )
                    # Optionally try repair_json here as well if initial parse fails
                    break

            if can_merge:
                intermediate_results[FINAL_MERGED_OUTPUT_KEY] = merged_data
            else:
                # Optionally store the errors?
                # intermediate_results[FINAL_MERGED_OUTPUT_KEY] = {"error": "Merge failed", "details": parse_errors}
                pass  # Add pass to fix empty else block

    # Return all results (including potentially merged final output)
    return intermediate_results


# Updated FUNCTION to orchestrate runs over samples and models
async def run_workflow_on_samples(
    sample_df: pl.DataFrame,
    models_to_test: List[str],
    workflow: List[PromptStage],  # Use the defined type
    available_prompts: Dict[str, str],
    input_formatters: Dict[str, Callable[[Dict[str, str]], str]],
    input_column: str = "formatted_context",  # Allow specifying input column
) -> List[Dict[str, Any]]:
    """
    Runs a defined workflow concurrently across multiple samples and models.
    Handles the potential automatically merged final output from execute_workflow.

    Args:
        sample_df: Polars DataFrame with input data.
        models_to_test: List of model names to run.
        workflow: The workflow definition.
        available_prompts: Map of prompt names to prompt text.
        input_formatters: Map of formatter names to functions.
        input_column: The column name in sample_df containing the initial input.

    Returns:
        A list of dictionaries, one per sample. Each dictionary contains:
            'input_data': The initial input for the sample.
            'model_outputs': A dict mapping model_name -> result_dict, where
                             result_dict contains task_id: output_str mappings
                             and potentially '_final_merged_output': merged_dict.
    """
    all_tasks = []
    task_metadata = []  # Stores sample_index, model, initial_input
    print("Preparing workflow tasks for all samples and models...")

    if input_column not in sample_df.columns:
        raise ValueError(f"Input column '{input_column}' not found in DataFrame.")

    for i, row in enumerate(sample_df.iter_rows(named=True)):
        initial_input_data = row[input_column]
        for model in models_to_test:
            task = asyncio.create_task(
                execute_workflow(  # Use the single-sample executor
                    model_name=model,
                    initial_input=initial_input_data,
                    workflow=workflow,
                    available_prompts=available_prompts,
                    input_formatters=input_formatters,
                )
            )
            all_tasks.append(task)
            task_metadata.append(
                {
                    "sample_index": i,
                    "model": model,
                    "initial_input_data": initial_input_data,
                }
            )

    print(f"Running {len(all_tasks)} workflow tasks concurrently...")
    # Each result is a dict: {prompt_name: output, potentially _final_merged_output: dict}
    all_individual_results: List[Dict[str, Any]] = await tqdm_asyncio.gather(
        *all_tasks, desc="Running Test Model Workflows", unit="task"
    )

    # --- Organize results by sample index ---
    print("Organizing workflow results by sample...")
    # results_by_sample will store structured results per sample
    results_by_sample: List[Dict[str, Any]] = [
        {"input_data": None, "model_outputs": {}} for _ in range(len(sample_df))
    ]

    for i, workflow_result_dict in enumerate(all_individual_results):
        meta = task_metadata[i]
        sample_index = meta["sample_index"]
        model = meta["model"]

        # Initialize sample dict if first time seeing this index
        if results_by_sample[sample_index]["input_data"] is None:
            results_by_sample[sample_index]["input_data"] = meta["initial_input_data"]

        # Store the entire result dictionary (including merged output key if present)
        # for this model under the 'model_outputs' key for the sample.
        results_by_sample[sample_index]["model_outputs"][model] = workflow_result_dict

    return results_by_sample
