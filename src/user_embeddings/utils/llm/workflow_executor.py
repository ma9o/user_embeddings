import asyncio
import json
from typing import Any, Dict, List, Optional, Tuple, TypedDict

import polars as pl
from pydantic import BaseModel, ValidationError
from tqdm.asyncio import tqdm_asyncio

from user_embeddings.utils.parsing import parse_llm_json_output

# Assuming get_text_completion is available via relative import or PYTHONPATH
# Adjust the import path based on your project structure if necessary
from .get_text_completion import get_text_completion


# --- Type Definitions ---
class TaskDefinition(TypedDict):
    """Defines a single task within a workflow stage."""

    prompt: str  # Name of the prompt module (also serves as task ID)
    input_from: Optional[List[str]]  # List of task IDs from PREVIOUS stages


class WorkflowStage(TypedDict):
    """Defines the structure for a single stage in a workflow."""

    stage: int
    tasks: List[TaskDefinition]  # Tasks to run in this stage


# --- Constants ---
# Removed FINAL_MERGED_OUTPUT_KEY as merging is no longer done explicitly here
# The output of a task is its validated data or an error indicator.


# --- Input Formatters ---
# Removed all input formatters and DEFAULT_INPUT_FORMATTERS


# --- Workflow Validation ---
def validate_workflow(
    workflow_name: str,
    workflow_definition: List[WorkflowStage],  # Updated type hint
    available_prompts: Dict[str, Tuple[str, str]],
    # Removed available_formatters
    available_output_models: Dict[
        str, type[BaseModel]
    ],  # Added for checking parsable tasks
) -> bool:
    """
    Validates a workflow definition against available prompts and output models.

    Checks for:
        - Existence of all specified prompt modules.
        - If a prompt module has a corresponding Pydantic model, it's considered parsable.
        - Correct dependencies (input_from tasks must exist in previous stages).

    Returns:
        True if the workflow is valid, False otherwise. Prints errors to console.
    """
    is_valid = True
    defined_outputs = (
        set()
    )  # Keep track of outputs (task IDs) generated by previous stages

    # Sort stages first to ensure correct dependency checking
    sorted_workflow = sorted(workflow_definition, key=lambda x: x["stage"])
    stage_numbers = {s["stage"] for s in sorted_workflow}
    if len(stage_numbers) != len(sorted_workflow) or sorted(stage_numbers) != list(
        range(1, len(sorted_workflow) + 1)
    ):
        print(
            f"Error: Workflow '{workflow_name}' has duplicate or non-sequential stage numbers: {sorted(stage_numbers)}"
        )
        is_valid = False

    all_task_ids_in_workflow = set()
    tasks_per_stage: Dict[int, List[str]] = {}

    for stage_def in sorted_workflow:
        stage_num = stage_def["stage"]
        current_stage_tasks = []

        if "tasks" not in stage_def or not stage_def["tasks"]:
            print(
                f"Error: Stage {stage_num} in workflow '{workflow_name}' has no tasks defined."
            )
            is_valid = False
            continue  # Skip further checks for this empty stage

        for task_def in stage_def["tasks"]:
            task_id = task_def.get("prompt")
            if not task_id:
                print(
                    f"Error: A task in stage {stage_num} of workflow '{workflow_name}' is missing the 'prompt' key."
                )
                is_valid = False
                continue

            current_stage_tasks.append(task_id)

            # Check for duplicate task IDs within the same stage (implicitly handled by later check if needed)
            if task_id in all_task_ids_in_workflow:
                print(
                    f"Error: Task ID '{task_id}' is defined multiple times in workflow '{workflow_name}'. Task IDs must be unique across all stages."
                )
                is_valid = False

            all_task_ids_in_workflow.add(task_id)

            # Validate prompt existence
            if task_id not in available_prompts:
                print(
                    f"Error: Prompt module '{task_id}' for task in stage {stage_num} "
                    f"of workflow '{workflow_name}' not found. "
                    f"Available prompts: {list(available_prompts.keys())}"
                )
                is_valid = False

            # Validate input_from dependencies
            input_from_tasks = task_def.get("input_from")
            if input_from_tasks:
                if stage_num == 1:
                    print(
                        f"Warning: Task '{task_id}' in stage 1 of workflow '{workflow_name}' specifies 'input_from', which is usually unnecessary for the first stage."
                    )
                    # Allow it, but warn. Might be valid if workflow starts conditionally.
                for required_input_task in input_from_tasks:
                    if required_input_task not in defined_outputs:
                        print(
                            f"Error: Task '{task_id}' in stage {stage_num} requires input from '{required_input_task}', "
                            f"but it's not generated by any previous stage in workflow '{workflow_name}'. "
                            f"Available previous outputs: {list(defined_outputs)}"
                        )
                        is_valid = False
                    # Check if the required input task is actually parsable if this task expects structured input
                    # This check might be too strict, maybe a task just needs the raw string.
                    # We'll rely on runtime checks for now.

        tasks_per_stage[stage_num] = current_stage_tasks
        # Add outputs of this stage to the set for next stage validation
        defined_outputs.update(current_stage_tasks)

    return is_valid


# --- Core Execution Logic ---
async def _run_single_prompt(model_name: str, prompt: str) -> str:
    """Helper to run a single model prompt and handle errors."""
    try:
        # This assumes get_text_completion is initialized elsewhere or handles it
        result = await get_text_completion(model_name, prompt)
        return result
    except Exception as e:
        print(f"Error running model {model_name} for prompt: {e}")
        # Return a standardized error string
        return f"ERROR: Model execution failed - {e}"


async def execute_workflow(
    model_name: str,
    initial_input: str,
    workflow: List[WorkflowStage],  # Updated type hint
    available_prompts: Dict[str, Tuple[str, str]],  # Map: name -> (text, version)
    available_output_models: Dict[str, type[BaseModel]],  # Added for parsing
) -> Dict[str, Any]:
    """Executes a defined workflow for a single model and initial input.
    Parses and validates the output of each task using corresponding Pydantic models
    if available in `available_output_models`.

    Args:
        model_name: The name of the LLM to use.
        initial_input: The starting input data for the first stage.
        workflow: List of stage definitions.
        available_prompts: Mapping of prompt module names to (prompt_text, version).
        available_output_models: Mapping of prompt module names to Pydantic models.

    Returns:
        A dictionary containing mappings from task_id (prompt name) to its result.
        The result can be:
            - A validated Pydantic model instance if parsing/validation succeeded.
            - The raw string output if no Pydantic model was defined.
            - An error string (e.g., "ERROR: Parse Failed", "ERROR: Validation Failed")
              if parsing/validation was attempted but failed.
            - An error string (e.g., "ERROR: Model execution failed", "ERROR: Missing required inputs")
              if the task couldn't run.
    """
    intermediate_results: Dict[str, Any] = {}  # Stores validated outputs or errors
    raw_outputs: Dict[str, str] = {}  # Stores raw LLM string outputs

    sorted_workflow = sorted(
        workflow, key=lambda x: x["stage"]
    )  # Ensure stages run in order

    for stage_def in sorted_workflow:
        stage_num = stage_def["stage"]
        tasks_in_stage: List[TaskDefinition] = stage_def["tasks"]

        # --- Prepare and run tasks concurrently within the stage ---
        stage_tasks_coroutines = []
        tasks_to_run_metadata: List[Dict[str, Any]] = []

        for task_def in tasks_in_stage:
            task_id = task_def["prompt"]
            input_from_tasks: Optional[List[str]] = task_def.get("input_from")
            current_task_input_str: str = ""
            skip_task = False
            error_msg = None

            # 1. Prepare input for the current task
            if stage_num == 1:
                if input_from_tasks:
                    print(
                        f"Warning: Task '{task_id}' in stage 1 specifies input_from, check workflow logic."
                    )
                    # Attempt to proceed assuming inputs might exist if workflow starts mid-way? Risky.
                    # For simplicity, assume initial_input is always used for stage 1 unless overridden.
                current_task_input_str = initial_input
            else:  # stage > 1
                if not input_from_tasks:
                    error_msg = (
                        f"ERROR: Task '{task_id}' in stage {stage_num} (model {model_name}) "
                        f"must specify 'input_from' listing tasks from previous stages."
                    )
                    print(error_msg)
                    skip_task = True
                else:
                    # Gather required inputs (validated objects/dicts or raw strings)
                    inputs_to_serialize = {}
                    all_inputs_available = True
                    for req_task_id in input_from_tasks:
                        if req_task_id not in intermediate_results:
                            all_inputs_available = False
                            error_msg = f"ERROR: Missing required input '{req_task_id}' for task '{task_id}' in stage {stage_num} (model {model_name})."
                            break
                        input_val = intermediate_results[req_task_id]
                        # Check if the required input itself is an error string
                        if isinstance(input_val, str) and input_val.startswith(
                            "ERROR:"
                        ):
                            all_inputs_available = False
                            error_msg = f"ERROR: Required input '{req_task_id}' for task '{task_id}' failed in a previous step."
                            break
                        # Store the actual value (could be Pydantic model, dict, or raw string)
                        inputs_to_serialize[req_task_id] = input_val

                    if not all_inputs_available:
                        print(error_msg)
                        skip_task = True
                    elif not inputs_to_serialize:
                        # This case shouldn't happen if input_from_tasks is not empty and all_inputs_available is True
                        error_msg = f"ERROR: Internal logic error - no inputs collected for task '{task_id}' despite 'input_from' being specified."
                        print(error_msg)
                        skip_task = True
                    else:
                        # Serialize the collected inputs into a JSON string
                        try:
                            # Convert Pydantic models to dicts for serialization
                            inputs_as_dicts = {}
                            for k, v in inputs_to_serialize.items():
                                if isinstance(v, BaseModel):
                                    inputs_as_dicts[k] = v.model_dump()
                                else:
                                    # Assume it's already serializable (dict, str, etc.)
                                    inputs_as_dicts[k] = v
                            current_task_input_str = json.dumps(
                                inputs_as_dicts, indent=2, ensure_ascii=False
                            )
                        except TypeError as e:
                            error_msg = f"ERROR: Failed to serialize inputs for task '{task_id}' in stage {stage_num}: {e}"
                            print(error_msg)
                            skip_task = True

            # 2. Create task coroutine if not skipped
            if skip_task:
                intermediate_results[task_id] = error_msg
                raw_outputs[task_id] = error_msg  # Also store error in raw
            else:
                try:
                    instruction_prompt_text = available_prompts[task_id][0]
                    model_prompt = (
                        f"{instruction_prompt_text}\n\nINPUT DATA:\n---\n"
                        f"{current_task_input_str}\n---"
                    )
                    stage_tasks_coroutines.append(
                        asyncio.create_task(
                            _run_single_prompt(model_name, model_prompt)
                        )
                    )
                    tasks_to_run_metadata.append({"task_id": task_id})
                except KeyError:
                    error_msg = f"ERROR: Prompt module name '{task_id}' not found in available_prompts."
                    print(error_msg)
                    intermediate_results[task_id] = error_msg
                    raw_outputs[task_id] = error_msg
                except Exception as e:
                    error_msg = f"ERROR: Failed to create task coroutine for '{task_id}' in stage {stage_num}: {e}"
                    print(error_msg)
                    intermediate_results[task_id] = error_msg
                    raw_outputs[task_id] = error_msg

        # 3. Gather raw results for the stage
        if stage_tasks_coroutines:
            stage_raw_results = await asyncio.gather(*stage_tasks_coroutines)
        else:
            stage_raw_results = []

        # 4. Process raw results: store raw output and attempt parsing/validation
        for i, raw_result_str in enumerate(stage_raw_results):
            task_id = tasks_to_run_metadata[i]["task_id"]
            raw_outputs[task_id] = raw_result_str  # Always store the raw output

            if raw_result_str.startswith("ERROR:"):
                intermediate_results[task_id] = raw_result_str  # Propagate error
                print(
                    f"Task '{task_id}' in stage {stage_num} failed execution for model {model_name}."
                )
                continue

            # Attempt parsing/validation only if a Pydantic model is defined
            output_model = available_output_models.get(task_id)
            if output_model:
                parsed_dict = parse_llm_json_output(raw_result_str, expect_type=dict)
                if parsed_dict is None:
                    intermediate_results[task_id] = "ERROR: Parse Failed"
                    print(
                        f"Warning: Failed to parse JSON output for task '{task_id}', model '{model_name}'. Raw output stored."
                    )
                else:
                    try:
                        validated_data: BaseModel = output_model.model_validate(
                            parsed_dict
                        )
                        intermediate_results[task_id] = (
                            validated_data  # Store the validated Pydantic object
                        )
                        # print(f"Task '{task_id}' output validated successfully.") # Debug log
                    except ValidationError as ve:
                        intermediate_results[task_id] = (
                            f"ERROR: Validation Failed - {ve}"
                        )
                        print(
                            f"Warning: Pydantic validation failed for task '{task_id}', model '{model_name}': {ve}. Raw output stored."
                        )
            else:
                # No model defined, store the raw string output as the result
                intermediate_results[task_id] = raw_result_str
                # print(f"Task '{task_id}' has no Pydantic model defined, storing raw output.") # Debug log

    # Return all results (validated models, raw strings, or errors)
    # Add raw outputs for debugging or potential fallback needs downstream
    return {"validated_outputs": intermediate_results, "raw_outputs": raw_outputs}


# Updated FUNCTION to orchestrate runs over samples and models
async def run_workflow_on_samples(
    sample_df: pl.DataFrame,
    models_to_test: List[str],
    workflow: List[WorkflowStage],  # Updated type hint
    available_prompts: Dict[str, Tuple[str, str]],  # Map: name -> (text, version)
    available_output_models: Dict[str, type[BaseModel]],  # Pass models for validation
    input_column: str = "formatted_context",  # Allow specifying input column
) -> List[Dict[str, Any]]:
    """
    Runs a defined workflow concurrently across multiple samples and models.
    Uses execute_workflow which handles parsing and validation internally.

    Args:
        sample_df: Polars DataFrame with input data.
        models_to_test: List of model names to run.
        workflow: The workflow definition.
        available_prompts: Map of prompt names to (prompt_text, version).
        available_output_models: Map of prompt names to Pydantic models.
        input_column: The column name in sample_df containing the initial input.

    Returns:
        A list of dictionaries, one per sample. Each dictionary contains:
            'input_data': The initial input for the sample.
            'model_outputs': A dict mapping model_name -> execution_result, where
                             execution_result contains:
                                - 'validated_outputs': {task_id: validated_pydantic_object | raw_string | error_string}
                                - 'raw_outputs': {task_id: raw_llm_output_string | error_string}
    """
    all_tasks = []
    task_metadata = []  # Stores sample_index, model, initial_input
    print("Preparing workflow tasks for all samples and models...")

    if input_column not in sample_df.columns:
        raise ValueError(f"Input column '{input_column}' not found in DataFrame.")

    for i, row in enumerate(sample_df.iter_rows(named=True)):
        initial_input_data = row[input_column]
        for model in models_to_test:
            task = asyncio.create_task(
                execute_workflow(  # Use the single-sample executor
                    model_name=model,
                    initial_input=initial_input_data,
                    workflow=workflow,
                    available_prompts=available_prompts,
                    available_output_models=available_output_models,  # Pass models
                )
            )
            all_tasks.append(task)
            task_metadata.append(
                {
                    "sample_index": i,
                    "model": model,
                    "initial_input_data": initial_input_data,
                }
            )

    print(f"Running {len(all_tasks)} workflow tasks concurrently...")
    # Each result is a dict: {"validated_outputs": {...}, "raw_outputs": {...}}
    all_individual_results: List[Dict[str, Any]] = await tqdm_asyncio.gather(
        *all_tasks, desc="Running Test Model Workflows", unit="task"
    )

    # --- Organize results by sample index ---
    print("Organizing workflow results by sample...")
    # results_by_sample will store structured results per sample
    results_by_sample: List[Dict[str, Any]] = [
        {"input_data": None, "model_outputs": {}} for _ in range(len(sample_df))
    ]

    for i, execution_result_dict in enumerate(all_individual_results):
        meta = task_metadata[i]
        sample_index = meta["sample_index"]
        model = meta["model"]

        # Initialize sample dict if first time seeing this index
        if results_by_sample[sample_index]["input_data"] is None:
            results_by_sample[sample_index]["input_data"] = meta["initial_input_data"]

        # Store the entire result dictionary (containing validated and raw outputs)
        # for this model under the 'model_outputs' key for the sample.
        results_by_sample[sample_index]["model_outputs"][model] = execution_result_dict

    return results_by_sample
