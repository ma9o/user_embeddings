# Implementation Considerations

This document outlines various design choices, trade-offs, and potential future considerations related to the project's implementation.

## Parallelism Strategy for Multi-Entity Processing

**Context:** When processing data for multiple independent entities (e.g., users, submissions, groups), a common pattern involves applying a core Dask computation to each entity.

**Approach 1: Sequential Orchestration of Parallel Tasks:**
Iterate through the entities using a standard Python loop. Within the loop, trigger the Dask computation graph for the current entity (e.g., using `.compute()` or `.persist().compute()`).
*   **Parallelism:** Dask effectively parallelizes the computation *within* each entity's task graph, distributing work across the cluster.
*   **Orchestration:** The Python loop manages the sequence, processing one entity's parallel workload after the previous one completes.

**Approach 2: Parallel Orchestration (`dask.delayed`):**
Wrap the per-entity processing logic (including triggering Dask computation and handling results/side effects) in a Python function decorated with `@dask.delayed`. Create a list of these delayed tasks for all entities and execute them concurrently using `dask.compute()`.
*   **Parallelism:** Dask parallelizes both the computation *within* each entity's task graph AND the *execution* of potentially overlapping task graphs for different entities.
*   **Orchestration:** Dask's scheduler manages the concurrent execution of the delayed tasks.

**Decision Rationale:**
While Approach 2 (`dask.delayed`) offers potentially higher concurrency by overlapping the processing of different entities, its benefits depend on cluster saturation. If the Dask computation for a single entity is complex enough to fully utilize the cluster resources, Approach 1 (Sequential Orchestration) is often sufficient and preferable due to:
*   **Simplicity:** Easier to implement, read, and debug.
*   **Resource Management:** Avoids potentially overwhelming the scheduler or memory with too many concurrent large graphs.
*   **Side Effects:** Simpler to manage operations like saving results for each entity.

Approach 1 is generally the recommended starting point. Consider Approach 2 if profiling reveals that the cluster is significantly underutilized during the sequential processing of entities.

---

## Data Formatting for `formatted_context`

The `formatted_context` column in the output CSV files generated by `src/user_embeddings/utils/dask_processing.py` contains conversation data structured as a **JSON string**.

**Format:**

*   The data follows a flexible, generic tree structure capable of representing various conversation types:
    *   **Threaded/Tree Structures (e.g., Slack, Discord, Reddit comments, Twitter threads):** Messages with replies/comments are nested naturally within the `replies` array.
    *   **Linear Chats (e.g., WhatsApp, Telegram, ChatGPT):** These are represented as a flat list of message objects at the top level, where each object's `replies` array is typically empty, or messages are simply appended chronologically to the top-level list.
    ```json
    [
      { // Root message 1 or first message in a linear chat
        "user": "...",
        "time": "DD-MM-YYYY HH:MM", // Or any standard timestamp format
        "content": "...",
        "replies": [
           { // Reply 1 to root message 1
             "user": "...",
             "time": "...",
             "content": "...",
             "replies": [...] // Further nested replies
           }
        ]
      },
      { // Root message 2 or second message in a linear chat
        "user": "...",
        "time": "...",
        "content": "...",
        "replies": [] // Empty if no replies or linear chat
      }
      // ... more messages
    ]
    ```
*   The specific fields (`user`, `time`, `content`) are illustrative; the actual fields might vary based on the data source.
*   The entire structure is serialized into a single JSON string using `json.dumps(..., indent=2)`.
*   When written to the CSV file, this JSON string is escaped according to standard CSV quoting rules. This means:
    *   The entire string is enclosed in outer double quotes (`"`...`"`).
    *   Any literal double quotes (`"`) within the JSON string are doubled (`""`).
    *   Newlines within the JSON string are preserved.

**Reasoning:**

*   Storing the context as a single JSON string preserves the hierarchical structure of the conversation within a single CSV field.
*   Using `indent=2` and standard CSV escaping makes the content somewhat human-readable directly within the CSV file for manual inspection or debugging.

**Usage with LLMs:**

*   **Crucially**, before feeding the `formatted_context` data to a Large Language Model (LLM) or any other process expecting the structured data, the string must be read from the CSV and parsed back into a Python object (or equivalent in other languages). Feeding the raw, CSV-escaped JSON string directly is inefficient (wastes tokens on escaping characters and indentation) and does not aid the LLM's comprehension.
*   In Python, this is typically done using `json.loads()`:
    ```python
    import json
    import pandas as pd

    # Assuming 'df' is a pandas DataFrame loaded from the CSV
    # Example for the first row:
    csv_escaped_json_string = df['formatted_context'].iloc[0]
    parsed_context = json.loads(csv_escaped_json_string)

    # 'parsed_context' now holds the Python list/dict structure
    # ready for further processing or LLM input.
    ```

*(Add more sections as the project evolves)*